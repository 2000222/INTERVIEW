### 梯度下降有哪些方法？
 - 随机梯度下降 SGD
 - 批量梯度下降 BGD
 - 小批量梯度下降 MBGD
 
批量梯度下降BGD：每次使用所有的样本来进行更新

 - 这样方便得到全局最优解
 - 但是很明显样本数多的时候难以实现

随机梯度下降SGD：每次只从全部样本中随机挑选出一个样本来更新参数

 - 样本很多的时候可能不需要全部的样本就能得到最优解
 - 噪音多，单个样本的影响大，可能会陷入局部最优解
 - 每次优化的方向不是向着全局最优去的
 
小批量梯度下降MBGD：

 - 每次更新使用一小部分样本
 - 为了克服上面两种方法的缺点，同时又兼顾两种方法的优点

### SGD中的S代表什么？
stochastic要求会读，代表随机，在深度学习里每次随机抽取一个batch的样本来更新参数。

### 监督学习 / 迁移学习 / 半监督学习 / 弱监督学习 / 非监督学习 概念？
 - 监督学习：利用已经有标签的数据来训练模型，使得模型可以对新的没有标签的样本进行预测
 - 迁移学习：将一个环境中学到的知识用来帮助新环境中的学习任务。
 - 无（非）监督学习：没有标签的数据进行分类，通常是聚类的方法，将相似度高的放到同一类中。
 - 半监督学习：有两个样本集，一个有标记 一个没有标记，综合利用有标记和没有标签的数据集进行学习。
 - 弱监督学习：标注的成本很高，精细的标注很难得到，比如这个图片是什么类别 框住物体的坐标，但是粗粒度的标签很容易得到 比如这个图片不是什么类别，物体大概在什么地方。或者是标签里的标注并不总是正确的。
 
### softmax loss推导？
softmax形如$ P(i) = \frac{e^{xj}}{\sum_ie^{xi}} $的函数。通过softmax可以将分类任务中输出的值转化成概率。P(i)中的i代表第i个样本，后面的i代表第i个类别，xj代表这个样本正确的类别的score

CrossEntropyLoss = $ -log(\frac{e^{xj}}{\sum_ie^{xi}}) $

 - 为什么要进行exp：这样可以将负的预测值转成正的。
 - 为什么要用预测某个类别的exp值比上所有的值之和：这样是为了计算P(j | x_i)概率 i是第i个样本 j是这个样本的真实类别，softmax loss可以看成是最大似然函数 这个函数希望正确的类别概率是1， 除以所有的exp值可以看成是标准化，让所有exp值都变成0-1之间，并且和为1。
 - 为什么最前面又要一个负号：因为这个是损失函数，用来度量分类器做的有多不好，最终计算的时候只计算-log(真实类别的概率)作为loss的值。当真实的概率接近1的时候，-log(p)就为0，当真实的概率很低的时候-log(p)就很高。
 - softmax loss的最大值和最小值是多少：最小是0， 最大是无穷。

**交叉熵求导**
设p为经过归一化之后的概率，fk是正确的类别的score，那么对于一个样本的loss就是$$ p_k = \frac{e^{f_k}}{\sum_je^{f_j}} \qquad L_i = -log(p_{y_i}) $$经过求导 简化最终对正确类别的输出score的导数是$$ \frac{\partial L_i}{\partial f_k} = p_k - \mathbb{1}(y_i=k) $$。不正确的概率的导数仍是概率本身。

假如说输出的概率是[0.2, 0.3, 0.5]中间的那个类别是正确的概率，那么这三个输出的神经元的导数就是[0.2, -0.7, 0.5]，这样子很直观，不正确的分类概率带来了loss，而正确分类则会降低loss。

### logistic 推导？
sigmoid函数作图就得到logistic曲线，sigmoid因变量取值范围为(-∞, +∞)输出为(0, +1)

$ sigmoid = \frac{1}{1+e^-x} $

sigmoid特点：

 - sigmoid是一个阈值函数 将输入限制在(0, 1)之间
 - sigmoid严格单调递增，而且其反函数(g(y)=x是反函数)也单调递增
 - sigmoid函数连续 处处光滑可导
 - sigmoid函数关于(0, 0.5)对称
 - f'(x) = F(f(x)) F(z) = z*(1-z)

### 为什么CNN也可以用在NLP中甚至于AlphaGo中也应用？CNN通过什么抓住了他们的共性？

以上的这几个应用都存在，**局部和整体的关系**。由低层次的特征经过组合成为高层次的特征，并且得到不同特征之间的空间相关性。如低层次的直线/曲线等特征，组合成为不同的形状，最后得到汽车的表示。

CNN抓住此特性的手段:

 - 局部连接，提取数据的局部特征
 - 权值共享，降低了网络的训练难度，一个Filter提取一个特征，在整个图片(语音、文本)中进行卷积
 - 池化操作，实现了数据的降维
 - 多层次结构，将低层次的局部特征组合成较高层次的特征

### 什么样的数据集不适合用深度学习？
 - 数据集太小。样本不足时深度学习相对其他的机器学习方法没有明显优势。
 - 数据集没有局部相关特性。深度学习在图像/语音/自然语言等领域优势明显，他们的一个共同的特点是局部相关性。图像中像素组成图片，语音中音位组成单词，文本中单词组成句子，这些特征元素的组合一旦被打乱，表示的含义也会变化。而对于没有局部相关性的数据，如传统的放贷预测模型中的各个特征互相不关联，打乱他们的顺序不影响结果。

### 对所有优化问题来说，有没有可能找到比现在已知算法更好的算法？
没有免费午餐定理：对于所有的问题，就算某个算法A效果比算法B更好，则必然存在一些问题算法B的效果更好。他们的期望性能相同。在实际的应用中，不同的场景会有不同的问题分布，所以**在优化算法时，针对具体问题进行分析，是算法优化核心所在**

### Dropout
**dropout是什么** 对于全连接神经网络单元，按照一定的概率使其暂时失效，故而每个mini-batch相当于在训练不同的神经网络，它强迫一个神经单元和随机挑选出来的其他神经单元共同工作，消除了神经元节点之间的联合适应性，增强了泛化能力，是CNN中防止过拟合的一个重要方法。

**dropout和bagging**：dropout可以认为是一种极端的Bagging，每个模型都在单独的数据上训练，同时，通过和其他模型对应参数的共享，从而实现模型参数的高度正则化。

**dropout有效的原因**：

1. 神经网络容易过拟合，除了使用dropout就是训练多个模型进行组合。
2. 但是训练多个神经网络耗费大量时间。
3. 每次进行完dropout都相当于从原网络采样出一个新的子网络。p=0.5时一层N个节点的网络，训练的参数数目不变，但是却相当于训练出了2^N个子网络。
4. 使用dropout相当于让这些子神经网络进行竞争。
5. 一般来说dropout的概率设置为0.5就可以，因为这个时候dropout随机生成的结构最多。
6. 数据量小的时候dropout效果不好，数据量大的时候效果好。
7. dropout的缺点是训练时间是没有dropout的2~3倍

### 什么是共线性？和过拟合有什么关系？
**共线性**：多变量线性回归中，输入的变量之间由于存在高度相关关系而使回归估计不准确。一个变量可以由其他的变量计算得到。共线性导致了输入神经网络或者其他回归模型的数据冗余，导致过拟合。

**为什么其他的学习器没有共线性？**：决策树和朴素贝叶斯中，前者的建模过程是逐步建立的，每次只有一个变量参与，这个建模机制导致了不会出现共线性。而朴素贝叶斯则直接假设各个输入的变量之间是独立的，因此也没有共线性的问题。

**如何消除共线性**：

 - 对输入的数据进行降维，排除相关性，比如PCA
 - 增加惩罚项，比如岭回归

### 广义线性模型是怎样被应用再深度学习中的？
**什么是广义线性模型**：Generalized Linear Model，是一种统计模型。普通的线性模型就是y=a0+a1x1+...+ε。在进行回归的时候，需要预测的y可能服从各种不同分布。比如普通的回归分析(预测房价 正态分布)，二项分布的预测(0-1分类 Logistic回归)，或者多项分布的预测(多类分类 softmax) 或者是泊松分布的预测(在某个时间点人流量的预测)等等。当预测的y的分布不属于正态分布的时候就需要其他的函数来作为联结函数来构成广义线性模型。比如sigmoid函数作联结函数来进行Logistic回归。

**为什么要建立统计模型**：为了研究自变量和因变量之间的关系，但当自变量能够100%决定因变量的时候就不需要统计模型了。使用统计模型是为了在有噪音的数据中找到规律。因为噪音(随机误差)也是有规律的。

**深度学习和广义线性模型关系**：

 - 深度学习从统计学角度，可以看做递归的广义线性模型
 - 广义线性模型相对于经典的线性模型，核心在于引入了联结函数g()
 - 深度学习看成广义线性模型的话，神经元的激活函数就相当于广义线性模型的联结函数
 - 不同的广义线性模型有不同的联结函数，[参考](http://blog.shakirm.com/2015/01/a-statistical-view-of-deep-learning-i-recursive-glms/)
 
### 什么造成了梯度消失/爆炸？
**梯度消失**：假如使用了sigmoid作为激活函数，当反向传播的时候，若传回来的数值是正数很大、或者负数很小，这个时候sigmoid的导数几乎为零$ f'(x) = f(x)(1 - f(x)) $。那么继续反向传播，梯度几乎为零，参数得不到更新，学习停止。使用tanh函数作为激活函数的时候也一样，梯度很容易变成接近零。

**梯度爆炸**：反向传播的时候当，一开始的梯度大于1，并且后续的梯度都大于1的时候，随着反向传播的进行，梯度会越来越大。sigmoid或者tanh的输出不是zero-center的，这会导致如果输入的数据x是正数$ f(w^Tx + b) $，那么对w的导数也都是正数或者都是负数。


**如何解决**

 - 使用ReLU/leakrelu/elu等作为激活函数，ReLU的结构导致了梯度不会消失，ReLU的导数恒等于1。ReLU在图像处理方面尤其胜过其他的激活函数。
 - batchnorm，反向传播的时候相当于把过大过小的数值，重新归一化到了0-1之间
 - 残差结构，残差结构出现以后可以轻松构建几百层 几千层的网络，就是因为shortcut保证了梯度不会消失过快。

### 各种激活函数进行对比
[聊一聊深度学习的activation function](https://zhuanlan.zhihu.com/p/25110450)

### 为什么要在神经网络中引入激活函数？
 - 增加神经网络的非线性，普通的矩阵相乘永远是线性的
 - 神经网络可以看做是用来模拟数据的分布函数，只需要一层神经网络加上非线性函数，可以理论上[模拟任何的连续函数](http://neuralnetworksanddeeplearning.com/chap4.html)。

### 什么是BatchNorm？
为了解决神经网络中出现梯度消失/爆炸的问题，提出了BatchNorm。每个BatchNorm针对每个channel（如果是全连接神经网络，那么一层学习$ \beta \gamma $）学习两个参数$ \beta \gamma $来对数据进行平移 缩放。极端的情况下这两个参数等于mini-batch的均值和方差，这个时候经过BN之后的数据和输入的数据相同。

BN对于不好的初始化有很高的鲁棒性，并且相当于是给每一层的输入都进行了标准化预处理，但是这种预处理以一种可以微分的形式嵌入了神经网络中。

**BN的计算过程**

1. 计算mini-batch的均值 $ \mu_B \leftarrow \frac{1}{m} \sum_{i=1}^m x_i $
2. 计算mini-batch的方差 $ \sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^m (x_i - \mu_B)^2 $
3. 进行标准化$ \hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} $
4. 利用$ \beta \gamma $进行平移缩放。$ y_i \leftarrow \gamma \hat{x_i} + \beta $

**预测时的BN**
训练的时候使用mini-batch，因此可以计算出均值和方差，但是预测时一次只有一个数据，所以均值方差要改变的都是0，那么BN层什么也不干，按照原样输出，这样是有问题的。解决的方法就是使用训练的所有数据中的均值和方差来做标准化。

### 神经网络中的稀疏性？
**稀疏性**：如果当神经元的输出接近于1的时候我们认为它被激活，当输出接近于0的时候认为它被抑制。那么使得神经元大部分的时间都是被抑制的限制被称为稀疏性限制。使用sigmoid的时候输出为0认为是抑制的，使用tanh输出为-1认为是抑制的，使用relu输出为0认为是抑制的。[参考Deep Sparse Rectifier Neural Networks](https://www.researchgate.net/publication/215616967_Deep_Sparse_Rectifier_Neural_Networks)

**稀疏性的体现**：

 - 使用ReLU激活函数，在前向传播的时候，让神经网络很好的得到了稀疏性，一部分的神经元被抑制，产生了不同的激活路径。其他的激活函数如sigmoid、tanh会同时对正数部分 负数部分都进行抑制，这也会导致梯度的问题。
 - 使用dropout，这样就更直接的对神经元进行失效，直接实现了稀疏性。

### 有哪些权重初始化方法？各适用于什么情况？
初始化对网络训练有一定的影响，避免在某一层的forward或者backward中进入饱和区域，拖慢神经网络的训练。初始化的指导思想：**为了使得神经网络不出现梯度消失/爆炸，每一层输出的方差应该尽量相等**。最著名也是最常用的就是Xavier uniform初始化方法，它通过输入和输出的神经元数目自动确定权重矩阵初始化大小，[推导参考](https://blog.csdn.net/Fire_Light_/article/details/79556192)。

**lecun_uniform**
从均匀分布$ [-limt, limt] $中进行采样，$ limt = \sqrt{\frac{3}{fan\_in}} $，其中$ fan\_in $是和这个权重矩阵进行相乘的神经网络单元数目，即输入的神经单元数目。

**glorot_normal/Xavier_normal**
以0为中心的截断正态分布中抽取样本。$ stddev = \sqrt{\frac{2}{fan\_in + fan\_out}} $。其中$ fan\_in $是输入的神经单元数，$ fan\_out $是输出的神经单元数

**glorot_uniform/Xavier_uniform**
从均匀分布$ [-limt, limt] $中进行采样，$ limt = \sqrt{\frac{6}{fan\_in + fan\_out}} $

**MSRA init**
出自何凯明[Delving Deep into Rectifiers:Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/pdf/1502.01852.pdf)。Xavier的推导是假设激活函数是线性的，但是显然常用的激活函数都不是线性的。这种方法也针对于ReLU进行初始化。
$$ w ～ G\Big[0, \sqrt{\frac{2}{n}}\Big] $$
$ G $为高斯分布，也就是正态分布。但是最常用的还是Xavier初始化方法。

### 为什么网络够深(Neurons 足够多)的时候，总是可以避开较差Local Optimal？
小的神经网络固然拥有较少的local optimal，但是它也总是更难以训练，并且小的神经网络的local optima的loss也更高。大的神经网络拥有更多的local optima，但是这些local optima更容易被克服，并且也比小神经网络的local optima表现的要好。参看[The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233)

### K-L散度是什么？
Kullback-Leibler Divergence，K-L散度是一种量化两种概率分布P和Q之间差异的方式，又称为相对熵。在概率和统计学上，经常会用一种更简单的近似分布来替代观察数据或太复杂的分布。K-L散度能够帮助我们度量使用一个分布来近似另一个分布时所损失的信息。
数据的熵Entropy，其中对数的底可以是任何的数字，当使用2的时候熵的值就表示用二进制来表示信息所需要的位数。
$$ H = -\sum_{i=1}^Np(x_i)logp(x_i) $$

K-L散度度量信息损失，如果使用2为底，则K-L散度的值表示用q近似p的时候信息损失的二进制位数
$$ D_{KL}(p||q) = \sum_{i=1}^Np(x_i)(logp(x_i) - logq(x_i)) $$
也可以写作：
$$ D_{KL}(p||q) = \sum_{i=1}^Np(x_i)log(\frac{p(x_i)}{q(x_i)}) $$

但是散度不是距离，$ D_{kl}(P||Q) \neq D_{kl}(Q||P) $。用Q近似P的散度值和用P近似Q的散度值不相同。
