## 线性分类器
线性分类器主要包含
1. score function：将原始数据映射到某个类别上。
2. loss function：计算预测结果和真实结果之间的误差。

假设在CIFAR-10数据集上进行分类，则score function线性映射 $ f(x_i, W, b) = W*x_i + b，从32*32* $3=3072维映射到K=10类上去，$ x_i $就是3072维的列向量，W是(10, 3072)的权重矩阵，b是10维的列向量它影响输出但是不直接和x相互作用。

注意到:
1. $ W*x_i $在10个类别上同时进行分类
2. 输入是固定的$ (x_i, y_i) $
3. 整个过程在学习W b

### 线性分类器-高维空间
可以认为线性分类器中的每行都是针对10个输出的某个类别的分类器，将这个类别和其他的数据分割开来。

### 线性分类器作为模版匹配
一个对线性分类器的解释就是W(输出的类别的个数, 输入的x的维度)中的每一行都是一个这个类别的模板。输入的数据和每个模板都作内积(点积)来找到响应值最高的那个

### bias技巧
可以在W中增加一列作为bias的值，同时输入的x中增加一个恒为1的特征。这样就不用再加上一列bias。

### 图像预处理
**center your data**，减去均值将数据的分布从[0, 255]转到[-127, 127]，更进一步将数据变为[-1, 1]。

## 损失函数
使用损失函数来衡量分类器的分类效果，也称作cost function。

### MultiClass SVM loss
一种常用的损失函数是**Multiclass Support Vector Machine loss**。它被称为SVM loss的原因是它希望数据的正确类别的score比其他的类别的score多一个Δ。准确来说，SVM 分类器使用的是hinge loss(也称为max-margin loss)

$ L_i = \sum \max(0, s_j - s_{yi} + Δ) $计算第i个样本的loss，$ score_j $是输出的正确类别的score，$ score_{yi} $是其他类别的score。

相比于MSE：
1. 如果出现一个较大的error，那么MSE的反向传播力度会很大，扩大的这个error的影响。
2. 只要正确类别的score相比其他的类别有了Δ，higine loss就不再关心这个样本了，而MSE则会一直进行优化。

### 正则化
对于能够正确分类的参数W不是唯一的，若W可以正确分类那么2W 3W...也能正确分类。在loss function上增加一个正则项$ 1/2R(W)=\sum_k \sum_l W_{kl}^2 $。L2 norm正则项。

$ L = 1/N * (\sum L_i) + λR(W) $data loss加上regularization loss

正则化的好处:
1. 压缩权重的值。
2. 对大权值的惩罚可以增加泛化的能力，减少过拟合。

比如w1=[1,0,0,0] w2=[0.25,0.25,0.25,0.25]可以得到相同的效果，但是增加了正则化那么w2会更守青睐，其实是因为w2利用了所有维度的数据。

一般不对bias进行正则化，bias并不直接和输入进行交互。加入了正则化之后loss永远也不可能变为0。

### 实践事项
**设置Δ** 一般不使用交叉验证来查找Δ的值，设置为1就可以。

**优化** 很多在NN中的函数都是不可导的比如max(0, x) 但是可以使用次梯度优化的方法解决

### Softmax classifier
另一种常用的分类器是softmax分类器。softmax是使得输出归一化变成概率的函数，这样交叉熵损失函数才能apply，一般而言不会说softmax loss。

**信息论解释** 交叉熵计算出来的分布可以用$ H(p, q) = - \sum p(x)log(q(x)) $来模拟。其中p(x)是标签的分布 q(x)是预测的分布。也可写成KL Diverse表示$ H(p, q) = H(p) + D_{kl}(p||q) $。

**概率预测** $ P(y | x, W) = e^{score_j} / \sum e^{score_i} $ softmax的输出可以看成是概率。

**数值稳定** 计算的时候$ e^{score_j} 、\sum e^{score_i} $也许会非常大，除以非常大的数字可能会引起数值不稳定，所以需要normalization trick。
```python
f = np.array([123, 456, 789]) # 假设有3个类别，预测的每个类别的输出都很大
p = np.exp(f) / np.sum(np.exp(f)) # exp之后数值会很大

# 将这些数字减去最大的，让所有数字<=0
f -= np.max(f) # f becomes [-666, -333, 0]
p = np.exp(f) / np.sum(np.exp(f)) # 不会导致数值不稳定
```

交叉熵$ L = -log(e^{score_j}/ \sum_i e^{score_i}) $特点：
 - 为什么要进行exp：这样可以将负的预测值转成正的。
 - 为什么要用预测某个类别的exp值比上所有的值之和：这样是为了计算$ P(j | x_i) $概率 i是第i个样本 j是这个样本的真实类别
 - softmax loss可以看成是最大似然函数 这个函数希望正确的类别概率是1。 
 - 除以所有的exp值可以看成是标准化，让所有exp值都变成0-1之间，并且和为1。
 - 为什么最前面又要一个负号：因为这个是损失函数，用来度量分类器做的有多不好，最终计算的时候只计算-log(真实类别的概率)作为loss的值。当真实的概率接近1的时候，-log(p)就为0，当真实的概率很低的时候-log(p)就很高。
 - softmax loss的最大值和最小值是多少：最小是0， 最大是无穷。

### softmax vs. SVM loss
当得到相同的score时，两个函数看score的角度不同:
1. SVM loss鼓励正确的类别比其他的类别多一个Δ
2. softmax则将输出的score 转换成概率，然后鼓正确的类别的概率要高一点。
当对同样的score使用两个损失函数的时候计算得到的loss可能不一样，但是这是没法做比较的，loss的数值只有使用同样的loss function的时候比较才有意义。

两个loss函数的性能其实差不多。SVM loss只关心那个margin，不关心数值具体是什么情况比如[10, -100, -100]。而softmax则会一直优化下去loss会不断降低。举个例子，对于SVM loss中的一个car classifier会把大部分精力放在区分car和trucks（更难），而不是区分car和frog（更容易）上。

### softmax中的概率
softmax分类器将所有输出的socre转化成所谓的“概率”，但是这个概率的数值是受到正则化的力度λ影响的，假如没有正则化的时候输出的socre为
[1, -2, 0] --> [e^1, e^-2, e^0] --> [2.71, 0.14, 1] --> [0.7, 0.04, 0.26]
然而使用了正则化之后，输出的score也许会减小一半
[0.5, -1, 0] --> [e^0.5, e^-1, e^0] --> [1.65, 0.37, 1] --> [0.55, 0.12, 0.33]
**这时概率变化了！**
当使用正则化的时候会使得输出的概率分布更加靠近均匀分布，所以**softmax输出的概率也只能作为参考，只能确定哪个类别可能性的排名，不是绝对的数值**