### Modeling one neuron
一开始神经网络是生物医学上对神经系统进行建模产生的，后来被应用到工程和机器学习上。大脑中的基本计算单元是神经元，一个人的神经系统中大约有860亿的神经元，这些神经元之间有大约10^14-10^15个突触。神经元可以看做是激活函数，突触可以看做是权重系数相乘。一个神经元可能收到多个突触传来的信息，同样一个激活函数可能同时收到多个权重系数相乘之和$ \sum_{i}w_{i}x_{i}+b $。经过神经元的激活，如果传来的激活信息达到了则会通过轴突继续向下发送信息，同样的若经过激活函数例如ReLU，没有被抑制则继续传播。

### Single neron as a linear classifier
一个简单的线性回归可以看作是一个神经元，他可以表现出对输入的喜欢（激活）或者不喜欢（抑制）。

**Binary Softmax classifier**：$ \sigma(\sum_{i}w_{i}x_{i} + b) $将输出转换成一个类别的概率$ P(y_i = 1 \mid x_i; w) $。而另一个类别的概率就表示为$ P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w) $。

**Binary SVM classifier**：使用max-margin loss来得到神经元的输出并进行训练，使之成为一个二分类SVM。

**Regularization interpretation**：在SVM softmax loss中的正则化部分在生物的角度可以看作是遗忘的作用，它会将某个伸向神经元的突触的权重变为接近0.

### Commonly used activation functions
**Sigmoid**
$ \sigma(x) = 1/(1 + e^{-x}) $。它接受一个实数并且将这个数压缩到0-1之间，特别大的数字压缩为1，特别小的数字压缩为0。现在通常不使用sigmoid，之前使用sigmoid是将它的输出为0表示为抑制状态，输出为1表示为激活状态。

 - Sigmoid会导致梯度消失。当数值特别大或者特别小的时候局部梯度会变成0，乘以反向传播回来的导数，导致了前面的参数无法得到更新。使用sigmoid的时候需要特别小心进行初始化，如果初始化不好就会梯度消失。
 - Sigmoid的输出不是以0为中心的，是以0.5为中心的。这个特性导致后续的神经元都会接收到不以0为中心的数据。如果输入神经元的数据都是正数$ f = w^Tx + b $，那么反向传播的时候对w的导数将一直是正数，或者一直是负数。这会导致**loss难以下降**。会让loss呈Z字形变化。但是一个batch的数据都进行更新某种程度上缓和了这个影响。总之这是个不方便但是影响也没那么大的坏处。
 
 **Tanh**
 双曲正切函数，也可以由sigmoid函数来表示：$ tanh(x) = a\sigma(2x)-1 $tanh将输入的实数压缩到-1到1之间，不同于Sigmoid，它的输出是以0为中心的zero-center。两者相比的话tanh更好。
 
**ReLU**
线性整流函数，Rectified Linear Unit非常流行,表达式为$ f(x) = max(0,x) $。
 - 它可以加速神经网络的训练。相比sigmoid和tanh它的训练速度更快
 - 计算量小。相比sigmoid tanh它的计算量更小
 - 但是会导致dead unit。如果输入的数据是负数，那么这个神经元将不再发挥作用，反向传播的时候也不会更新参数。如果学习率太高，在训练的时候可能高达40%的神经元会dead（这些神经元对整个数据集都不会激活）。

**Leaky ReLU**
Leaky ReLU试图去修复ReLU中抑制神经元的问题，若输入的数据是负数，Leaky ReLU将不会给它一个0的导数，而是会给一个很小的导数如0.01。在有些数据集上Leaky ReLU取得了较好的效果，但是并不总是比ReLU好。还有[PReLU](https://arxiv.org/abs/1502.01852)

**Maxout**
表达式为$ max(w_1^Tx+b_1, w_2^Tx+b_2) $，可以看到ReLU和Leaky ReLU是这个函数的特殊形式（对于ReLU w1，b1=0）它拥有ReLU的所有优势，并且没有它的缺点。但是这个激活函数会导致计算量增大。

**应该使用什么激活函数**
一般可以直接试一试ReLU，并且注意学习率和dead的神经元。如果担心dead的神经元的话可以试一试Leaky ReLU或者Maxout。千万不要使用Maxout，也许可以尝试tanh，但是它的效果一定会比较差的。

### 神经网络架构
**Naming conventions**当说N层神经网络的时候一般不算上输入层，logistic回归和SVM分类是特殊的单层神经网络。多层神经网络有时候也被称为ANN MLP。为了不和大脑的神经系统混淆，一般把神经元neuron称作单元unit。

**Output Layer**输出层一般没有激活函数，或者认为他们有一个恒等的激活函数。这是因为最后一层需要输出例如分类的score，不能被squash，或者是用于回归最后也不能被squash。

**Sizing neural networks**衡量神经网络大小的标准一般是衡量神经网络中的神经元有多少，或者更长见的是衡量可学习的参数有多少。现代的CNN一般有1亿个参数，10-20层。

### 神经网路的表示能力
理论上来说单层的神经网络加上一个非线性激活函数可以[模拟任何的连续函数](http://neuralnetworksanddeeplearning.com/chap4.html)。为什么单层的神经网络就可以模拟任何的函数，现在的很多任务都用很深的神经网络。因为单层的神经网络从理论上是可以模拟，但是实现的时候是两层神经网络比一层好。通常3层神经网络比2层好，但是更深的话就没有更好的表现了。但是再卷积神经网络中是越深越好。

### 使用什么样的神经网络架构
当增加神经网络的层数或是增加更多的隐藏层神经元的时候，它的容量capacity也增加了。它所能表示的函数范围也增加了，但是也更容易过拟合。**过拟合**一个模型拥有很高的容量，它同时学会了数据中的噪音而不仅仅是数据之间的关系。

似乎当数据没那么复杂的时候小型的网络更受青睐，但实际上小型的神经网络更难以进行训练，很容易陷入局部最优解，一般采用的是较大的神经网络并采取一些正则化手段如L2-norm，而不是减少神经元的数量来控制过拟合。

小的神经网络有相对少的局部最优解，大的神经网络虽然有更多的局部最优解，但是它更容易克服这一点，更容易从中走出来。参考[ The Loss Surfaces of Multilayer Networks](https://arxiv.org/abs/1412.0233)。并且大的神经网络的全局最优比小的神经网络的全局最优更好。