### CNNs/ConvNets
卷积神经网络和普通的神经网络的结构很一样，同样是由神经元构成，同样有可以学习的weights和bias。每个神经元接受输入进行点积并输入激活函数。整个网络表现的也像是一个可微分的score function，从由像素组成的图片映射到类别标签。然后再有一个loss function。

普通的神经网络的一个神经元由数据（weights bias）和操作组成（作点击，进行激活计算），最后的一层称为输出层，在分类人物中输出层输出的是每个类别的score。

普通的神经网络并不能很好的利用图片的scale，在cifar-10中图片的格式是32x32x3。普通的神经网络会把图片展开成3072维的向量，并在第一层设置3072个权重作点积。如果是200x200x3的图片那么将会有120000个权重，这很明显太多了。

**3D形式的神经元**
卷积神经网络利用了一点，那就是输入是一个3D的矩阵。卷积神经网络的神经元由width、height、depth描述大小。每个神经元和输入的一小块区域作点积，求和，然后再进行激活。除了连接形式不一样之外，其他的和原来的神经元表现是一样的，先作点积，然后求和，然后激活

### CNNs中使用的Layers
卷积神经网络中使用的Layer通常有卷积层 池化层 全连接层。CNNs中的每一层并不总是有参数，如池化层 激活层没有参数，卷积层 BN层有参数。卷积神经网络中也并不是每一层都有超参数，激活层就没有。

**卷积层**
卷积层是卷积神经网络中耗费最多计算量的地方。卷积核在图片上进行滑动，计算出一个2d的activation map，这个activation map中代表卷积核在每个位置上响应程度。直观地来说，每个卷积核都在学习一个局部特征，在每个位置上激活程度高的代表了在这个位置上存在这个特征的可能性高。这些特征可以是一些局部的曲线，或者局部的颜色块。将多个activation map堆叠在一起就组成了新的3D的feature map。

*local connectivity*。卷积核只连接输入feature map的一小块区域。这种局部连接的大小范围也是一种超参数称为神经元的感受域receptive field（一般用width*height描述）。感受域只在width height是局部的，但是连接这这片区域后面的所有depth。

*Spatial arrangement*。控制卷积层的输出的超参数是depth stride zero-padding。

 - depth对应的是卷积核的个数。一般称呼注视着同一片区域的卷积核为depth column也称为fibre。
 - stride，卷积核在feature map上进行滑动时的步长。
 - zero-padding，在输入feature map上添加zero的边的数目，可以通过添加zero-padding使得输入和输出的width height相同。

如果stride为1，那么可以使用公式$ P = (F-1)/2 $来计算zero-padding的数目使得输入输出相同。

输出尺度的计算公式$ \left \lfloor (W + 2P - D(F-1) -1)/S + 1 \right \rfloor $其中$ W $是输入的尺度，$ P $是zero-padding，$ F $是卷积核的尺度，$ S $是步长stride，$ D $是dilation膨胀程度。

在卷积层中对于每个卷积核划过的位置都认为是存在一个神经元（输出的同一个channel中的每个位置），不过这些神经元共享了同一套的参数。

但也要注意，有时候这种权重共享的形式并不合理。当输入的图片是某种中心化的结构的时候，如人脸一般就是在图片的中心。那么普通的卷积层就只能学习到发型或者其他的局部东西，这个时候可以稍微放松一点局部连接的规则。并且称这种改过的神经网络为Locally-Connected Layer。

**用矩阵乘法实现卷积**

 - *im2col*将卷积核滑过的每个小块区域变成一个列向量，假如输入是(227, 227, 3)并且使用(11, 11, 3)的卷积核stride为4，那么原图上滑动过的每个位置变成列向量之后为11*11*3=363维，一行有(227-11)/4+1=55个，55*55=3025，那么就得到了X_col(363, 3025)的矩阵。
 - *展开卷积核*如果有96个(11, 11, 3)的卷积核，那么就有W_row(96, 363).
 - *进行卷积*卷积的结果就是np.dot(W_row, X_col)输出是(96, 3025)
 - *reshape*输出可以重新reshape到(55, 55, 96)
这个方法的缺点就是浪费很多的内存，但另一方面这样计算很高效，并且im2col的思想也可以用在池化层中。

**反向传播**
同样反向传播也是卷积的形式。假如在feature map M0上进行卷积得到feature map M1，卷积核大小为k_s，那么M1[0, 0]进行反向传播，先将梯度复制成k_s*k_s份，对应位置相乘后的梯度，然后每个对应位置求对W的偏导，求对M0[i,j]的偏导。同时因为卷积核在整个M0上都进行了卷积，所以最终对W的参数是各个位置上的梯度相加。

**Dilated conv**
在卷积核的每个位置之间可以有空洞，这被称为空洞卷积。空洞卷积可以扩大感受域。

### 池化层
在连续的卷积层中插入池化层可以降低特征的维度，并且可以控制过拟合。池化层在每个channel中单独进行操作，在池化层中使用zero-padding也很常见。最大池化是一种常见的池化操作，并且使用filter 2x2 步长为2 过滤掉75%的数据。有两种常见的池化参数：filter为3x3 步长为2、filter为2步长为2.

除了最大池化还有average pooling、L2-norm pooling。average pooling过去很常用但是最近它的效果经研究不如最大池化。

**反向传播**
池化的反向传播，如果是最大池化那么将导数传播到最大的那个值即可，在前向传播的时候需要记住最大的那个值的下标。如果是average pooling那么把导数平均分给每个值即可。

**不使用池化层**
一些人不喜欢池化操作，认为这损失了信息，并且主张使用大的卷积核来实现对特征的降维。但是使用池化是未来的趋势，并且可以加速训练。

### Normalization layer
卷积神经网络中使用了很多的normalization 层，但是这些normalization层的效果并不是很好。现在使用最多的还是batch normalization。

### 全连接层转为卷积层
全连接层和卷积层的区别就是局部连接、权值共享了。但是这些层中仍然是计算点积求和激活，所以两者可以进行连接：
先卷积变成类似(1, 1, 4096)，然后使用1000个(1, 1, 4096)的卷积核变为需要的输出(1, 1, 1000)。

使用卷积层代替全连接层还有一个好处：假如训练的时候输入是(224, 224, 3)经过前面的卷积变成(7, 7, 512)然后7x7卷积变成类别数。测试的时候输入可能是(384, 384, 3)那么经过卷积变成(12, 12, 512)然后7x7卷积变成(6, 6, 类别数)这个6x6的feature map相当于对应原来图片的缩小版，每个位置相当于在原图上滑动做crop然后输入神经网络，这比在原图片上进行滑动更高效。如果是分类的话，这36个位置做平均进行输出。衍生出来的一个技巧就是将原图扩大，然后输入神经网络，相当于在每个位置上都进行卷积计算score最后平均，这样效果更好。

但是如果原图很小呢，经过前面的卷积feature map size已经小于7了，forwarding the converted ConvNet twice: First over the original image and second over the image but with the image shifted spatially by 16 pixels along both width and height.

### 神经网络的结构
一般来说是```INPUT -> [[CONV -> RELU]*N -> POOL]*M -> [FC -> RELU]*K -> FC```一般来说其中```0 <= N <= 3```并且```0 <= K < 3```。

一般来说**堆叠很多小的卷积核比直接使用一个大的卷积核要好**，堆叠三层3x3的卷积层，相对于初始的feature map来说第一层的感受域是3x3，第二层的感受域就是5x5，第三层就是7x7。好处与坏处：

 - 直接一层7x7的卷积相当于只进行了一次线性的计算然后输入非线性的激活函数，而三层的话其中就夹带有非线性，这会使得输出的特征更有表现力（特征更高级）
 - 假如有C个channel，那么7x7卷积有$ C \times (7 \times 7 \times C) = 49C^2 $个参数，而三个3x3卷积就只有$ 3 \times (C \times (3 \times 3 \times \timesC)) = 27C^2 $个参数。这样参数更少，并且表示能力也更强。
 - 但是这样也会浪费内存去存储中间的输出

在实践中最好是使用在ImageNet上表现的不错的神经网络。自己尝试设计新的神经网络可能会有各种的问题，最好是借鉴各种比赛的前几名。

**每层的模式**
输入层，也就是输入的图片的高和款一般是2的倍数，如32 64 96 224 384 512等。

卷积层应该使用小的卷积核3x3 5x5，尽量使用1步长。如果非要用7x7的大卷积核的话，那一般也就是在第一个卷积层使用。

池化层一般使用最大池化，并且使用2x2filter，步长为2.或者3x3 filter步长为2.一般不使用大于3的filter，这将会导致很差的表现。

为什么步长一般为1？小的步长表现更好，并且卷积的时候使用小步长不改变feature map的size，让max pool改变size。

为什么进行padding？保持feature map的size不变。如果不进行padding，那么feature map越来越小，图像边缘的信息经过几次卷积就消失掉了。

### 训练时候所需要的计算资源
现代的GPU内存通常为3/4/6GB，最大的也就是128GB。为了有效利用内存，应该注意以下数据：

 - 中间层的输出。中间的卷积层以及激活层等等，这些输出需要保存为了进行反向传播。在训练完进行测试的时候就不用保存中间层的输出了。
 - 保存参数。当使用动量或者Adagrad RMSProp的时候存储参数所需要的内存更多，为了训练存储参数相关的内存需要是参数的三倍以上。
 - 各种其他的存储。如输入的图片需要存储，或者他们的data argumentation。
当计算完参数的个数，把这个数转成G为单位，如果是单精度float乘以4，如果是双精度double乘以8，换算成GB。如果内存不够可以减少batch size。

### 迁移学习
利用之前在其他数据集上学习到的知识在新的数据集上进行微调。当新的数据集较大的时候，可能只需要重新训练最后一个全连接层，而前面的层都冻结起来。当新的数据集较大的时候需要重新训练最后的所有全连接层（一般是三个）并且前面的卷积层也都需要用很小的学习率进行学习。
|  | 较相同的数据集 | 较不同的数据集 |
| 很少的数据 | 重新训练最后一个全连接层 | 很麻烦，尝试从不同的地方开始重新训练 |
| 很多的数据 | 微调全部层 | 微调全部层 |

**注意事项**
进行迁移学习的时候要注意一下几点：

 - *原有模型的约束*。如果使用一个已经预训练的模型，那么会受限于这个模型。比如不能从这个模型中剥离某层，或者插入几层。但是如果是输入图片的尺寸大一点的话完全可以使用预训练的模型，因为权值共享，就相当于在原图上进行滑动了。
 - *学习率*。使用预训练的模型，对于前面的卷积层，使用较小的学习率。因为我们的期望是前面的卷积层已经学习到了足够的知识，所以我们并不像过于扰动他们。特别是新加上的全连接层是随机初始化的，这可能会导致一开始有很大的loss。